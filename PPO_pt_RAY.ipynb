{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "##### Model construction #####\n",
    "def mlp(odim=24, hdims=[256,256], actv=nn.ReLU(), output_actv=None):\n",
    "    layers = []\n",
    "    prev_hdim = odim\n",
    "    for hdim in hdims[:-1]:\n",
    "        layers.append(nn.Linear(prev_hdim, hdim, bias=True))\n",
    "        layers.append(actv)\n",
    "        prev_hdim = hdim\n",
    "    layers.append(nn.Linear(prev_hdim, hdims[-1]))\n",
    "    if output_actv is None:\n",
    "        return nn.Sequential(*layers)\n",
    "    else:\n",
    "        layers.append(output_actv)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, odim, adim, hdims=[64,64], actv=nn.ReLU(), output_actv=None):\n",
    "        super(CategoricalPolicy, self).__init__()\n",
    "        self.output_actv = output_actv\n",
    "        self.net = mlp(odim, hdims=hdims, actv=actv, output_actv=output_actv)\n",
    "        self.logits = nn.Linear(in_features=hdims[-1], out_features=adim)\n",
    "    def forward(self, x, a=None):\n",
    "        output = self.net(x)\n",
    "        logits = self.logits(output)\n",
    "        if self.output_actv:\n",
    "            logits = self.output_actv(logits)\n",
    "        prob = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs=prob)\n",
    "        pi = dist.sample()\n",
    "        logp_pi = dist.log_prob(pi)\n",
    "        logp = dist.log_prob(a)\n",
    "        return pi, logp, logp_pi, pi\n",
    "\n",
    "class GaussianPolicy(nn.Module):    # def mlp_gaussian_policy\n",
    "    def __init__(self, odim, adim, hdims=[64,64], actv=nn.ReLU(), output_actv=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.output_actv = output_actv\n",
    "        self.mu = mlp(odim, hdims=hdims+[adim], actv=actv, output_actv=output_actv)\n",
    "        self.log_std = nn.Parameter(-0.5*torch.ones(adim))\n",
    "    def forward(self, x, a=None):\n",
    "        mu = self.mu(x)\n",
    "        std = self.log_std.exp()\n",
    "        policy = Normal(mu, std)\n",
    "        pi = policy.sample()\n",
    "        # gaussian likelihood\n",
    "        logp_pi = policy.log_prob(pi).sum(dim=1)\n",
    "        if a is not None:\n",
    "            logp = policy.log_prob(a).sum(dim=1)\n",
    "        else:\n",
    "            logp = None\n",
    "        return pi, logp, logp_pi, mu        # 순서 ActorCritic return 값이랑 맞춤.\n",
    "\n",
    "class ActorCritic(nn.Module):   # def mlp_actor_critic\n",
    "    def __init__(self, odim, adim, hdims=[64,64], actv=nn.ReLU(),\n",
    "                 output_actv=None, policy=None, action_space=None):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        if policy is None and isinstance(action_space, Box):\n",
    "            self.policy = GaussianPolicy(odim, adim, hdims, actv, output_actv)\n",
    "        elif policy is None and isinstance(action_space, Discrete):\n",
    "            self.policy = CategoricalPolicy(odim, adim, hdims, actv, output_actv)\n",
    "        self.vf_mlp = mlp(odim, hdims=hdims+[1],\n",
    "                          actv=actv, output_actv=output_actv)\n",
    "    def forward(self, x, a=None):\n",
    "        pi, logp, logp_pi, mu = self.policy(x, a)\n",
    "        v = self.vf_mlp(x)\n",
    "        return pi, logp, logp_pi, v, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym,pybullet_envs,time, psutil, torch\n",
    "\n",
    "print(\"Pytorch version:[%s].\"%(torch.__version__))\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(\"device:[%s].\"%(device))\n",
    "\n",
    "class PPOAgent():\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.env, self.eval_env = get_envs()\n",
    "        odim = self.env.observation_space.shape[0]\n",
    "        adim = self.env.action_space.shape[0]\n",
    "\n",
    "        # Actor-critic model\n",
    "        ac_kwargs = dict()\n",
    "        ac_kwargs['action_space'] = self.env.action_space\n",
    "        self.actor_critic = ActorCritic(odim, adim, self.config.hdims,**ac_kwargs)\n",
    "        self.buf = PPOBuffer(odim=odim,adim=adim,size=self.config.steps_per_epoch,\n",
    "                             gamma=self.config.gamma,lam=self.config.lam)\n",
    "\n",
    "        # Optimizers\n",
    "        self.train_pi = torch.optim.Adam(self.actor_critic.policy.parameters(), lr=self.config.pi_lr)\n",
    "        self.train_v = torch.optim.Adam(self.actor_critic.vf_mlp.parameters(), lr=self.config.vf_lr)\n",
    "\n",
    "        # model load\n",
    "        #self.actor_critic.load_state_dict(torch.load('model_data/model_weights'))\n",
    "\n",
    "    def update_ppo(self):\n",
    "        self.actor_critic.train()\n",
    "\n",
    "        obs, act, adv, ret, logp = [torch.Tensor(x) for x in self.buf.get()]\n",
    "\n",
    "        obs = torch.FloatTensor(obs)\n",
    "        act = torch.FloatTensor(act)\n",
    "        adv = torch.FloatTensor(adv)\n",
    "        ret = torch.FloatTensor(ret)\n",
    "        logp_a_old = torch.FloatTensor(logp)\n",
    "\n",
    "        # Policy gradient step\n",
    "        for i in range(self.config.train_pi_iters):\n",
    "            _, logp_a, _, _ = self.actor_critic.policy(obs, act)\n",
    "            # pi, logp, logp_pi, mu\n",
    "\n",
    "            # PPO objectives\n",
    "            ratio = (logp_a - logp_a_old).exp()\n",
    "            min_adv = torch.where(adv > 0, (1 + self.config.clip_ratio) * adv,\n",
    "                                  (1 - self.config.clip_ratio) * adv)\n",
    "            pi_loss = -(torch.min(ratio * adv, min_adv)).mean()\n",
    "\n",
    "            self.train_pi.zero_grad()\n",
    "            pi_loss.backward()\n",
    "            self.train_pi.step()\n",
    "\n",
    "            kl = torch.mean(logp_a_old - logp_a)\n",
    "            if kl > 1.5 * self.config.target_kl:\n",
    "                break\n",
    "\n",
    "        # Value gradient step\n",
    "        for _ in range(self.config.train_v_iters):\n",
    "            v = self.actor_critic.vf_mlp(obs).squeeze()\n",
    "            v_loss = F.mse_loss(v, ret)\n",
    "\n",
    "            self.train_v.zero_grad()\n",
    "            v_loss.backward()\n",
    "            self.train_v.step()\n",
    "\n",
    "    def main(self):\n",
    "        start_time = time.time()\n",
    "        o, r, d, ep_ret, ep_len, n_env_step = self.env.reset(), 0, False, 0, 0, 0\n",
    "\n",
    "        self.actor_critic.eval()\n",
    "\n",
    "        # Main loop: collect experience in env and update/log each epoch\n",
    "        for epoch in range(self.config.epochs):\n",
    "            if (epoch == 0) or (((epoch + 1) % self.config.print_every) == 0):\n",
    "                print(\"[%d/%d]\" % (epoch + 1, self.config.epochs))\n",
    "            for t in range(self.config.steps_per_epoch):\n",
    "                a, _, logp_t, v_t, _ = self.actor_critic(\n",
    "                    torch.Tensor(o.reshape(1, -1)))  # pi, logp, logp_pi, v, mu\n",
    "\n",
    "                o2, r, d, _ = self.env.step(a.detach().numpy()[0])\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "                n_env_step += 1\n",
    "\n",
    "                # save and log  def store(self, obs, act, rew, val, logp):\n",
    "                self.buf.store(o, a, r, v_t, logp_t)\n",
    "\n",
    "                # Update obs (critical!)\n",
    "                o = o2\n",
    "\n",
    "                terminal = d or (ep_len == self.config.max_ep_len)\n",
    "                if terminal or (t == (self.config.steps_per_epoch - 1)):\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    last_val = 0 if d else self.actor_critic.vf_mlp(torch.Tensor(o.reshape(1, -1))).item()\n",
    "                    self.buf.finish_path(last_val)\n",
    "                    o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            # Perform PPO update!\n",
    "            self.update_ppo()\n",
    "\n",
    "            # # save model\n",
    "            # if epoch % 10 == 0:\n",
    "            #     torch.save(self.actor_critic.state_dict(), 'model_data/model_weights')\n",
    "            #     print(\"Weight saved\")\n",
    "\n",
    "            # Evaluate\n",
    "            self.actor_critic.eval()\n",
    "            if (epoch == 0) or (((epoch + 1) % self.config.evaluate_every) == 0):\n",
    "                ram_percent = psutil.virtual_memory().percent  # memory usage\n",
    "                print(\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\" %\n",
    "                      (epoch + 1, self.config.epochs, epoch / self.config.epochs * 100,\n",
    "                       n_env_step,\n",
    "                       time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)),\n",
    "                       ram_percent)\n",
    "                      )\n",
    "                o, d, ep_ret, ep_len = self.eval_env.reset(), False, 0, 0\n",
    "                _ = self.eval_env.render(mode='human')\n",
    "                while not (d or (ep_len == self.config.max_ep_len)):\n",
    "                    a, _, _, _ = self.actor_critic.policy(torch.Tensor(o.reshape(1, -1)))\n",
    "                    o, r, d, _ = self.eval_env.step(a.detach().numpy()[0])\n",
    "                    _ = self.eval_env.render(mode='human')\n",
    "                    ep_ret += r  # compute return\n",
    "                    ep_len += 1\n",
    "                print(\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\" % (ep_ret, ep_len))\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "        self.env.close()\n",
    "        self.eval_env.close()\n",
    "\n",
    "    def test(self):\n",
    "        gym.logger.set_level(40)\n",
    "        _, eval_env = get_envs()\n",
    "        o, d, ep_ret, ep_len = eval_env.reset(), False, 0, 0\n",
    "        _ = eval_env.render(mode='human')\n",
    "        while not (d or (ep_len == self.config.max_ep_len)):\n",
    "            a, _, _, _ = self.actor_critic.policy(torch.Tensor(o.reshape(1, -1)))\n",
    "            o, r, d, _ = eval_env.step(a.detach().numpy()[0])\n",
    "            _ = eval_env.render(mode='human')\n",
    "            ep_ret += r  # compute return\n",
    "            ep_len += 1\n",
    "        print(\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "              % (ep_ret, ep_len))\n",
    "        eval_env.close()  # close env\n",
    "\n",
    "\n",
    "def get_envs():\n",
    "    env_name = 'AntBulletEnv-v0'\n",
    "    env,eval_env = gym.make(env_name), gym.make(env_name)\n",
    "    _ = eval_env.render(mode='human') # enable rendering on test_env\n",
    "    _ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering\n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return env,eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x\n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics for.\n",
    "        with_min_and_max (bool): If true, return min and max of x in\n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "    if with_min_and_max:\n",
    "        global_min = (np.min(x) if len(x) > 0 else np.inf)\n",
    "        global_max = (np.max(x) if len(x) > 0 else -np.inf)\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute discounted cumulative sums of vectors.\n",
    "    input:\n",
    "        vector x, [x0, x1, x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,\n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, odim, adim, size=5000, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, adim), dtype=np.float32)\n",
    "        self.act_old_buf = np.zeros(combined_shape(size, adim), dtype=np.float32) # added\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size  # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "\n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size  # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf,\n",
    "                self.ret_buf, self.logp_buf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ray Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime,gym,os,pybullet_envs,time,os,psutil,ray\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Rollout Worker\n",
    "def get_env():\n",
    "    return gym.make('AntBulletEnv-v0')\n",
    "\n",
    "def get_eval_env():\n",
    "    eval_env = gym.make('AntBulletEnv-v0')\n",
    "    if RENDER_ON_EVAL:\n",
    "        _ = eval_env.render(mode='human') # enable rendering\n",
    "    _ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering\n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return eval_env\n",
    "\n",
    "class RolloutWorkerClass(object):\n",
    "    def __init__(self, seed=1):\n",
    "        self.seed = seed\n",
    "        self.env = get_env()\n",
    "        odim, adim = self.env.observation_space.shape[0], self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "\n",
    "        # Initialize PPO\n",
    "        # Actor-critic model\n",
    "        ac_kwargs = dict()\n",
    "        ac_kwargs['action_space'] = self.env.action_space\n",
    "        self.model = ActorCritic(odim, adim, hdims, **ac_kwargs)\n",
    "\n",
    "        # # model load\n",
    "        # self.model.load_state_dict(torch.load('model_data/model_weights'))\n",
    "        # print(\"weight load\")\n",
    "\n",
    "        # Initialize model\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # Optimizers\n",
    "        self.train_pi = torch.optim.Adam(self.model.policy.parameters(), lr=pi_lr)\n",
    "        self.train_v = torch.optim.Adam(self.model.vf_mlp.parameters(), lr=vf_lr)\n",
    "\n",
    "    def get_weights(self):\n",
    "        weight_vals = self.model.state_dict()\n",
    "        return weight_vals\n",
    "\n",
    "    def set_weights(self, weight_vals):\n",
    "        return self.model.load_state_dict(weight_vals)\n",
    "\n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self, worker_id=0, ep_len_rollout=1000):\n",
    "        # Parse\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "\n",
    "        # Each worker should maintain its own environment\n",
    "        import pybullet_envs, gym\n",
    "        gym.logger.set_level(40)  # gym logger\n",
    "        self.env = get_env()\n",
    "        odim, adim = self.env.observation_space.shape[0], self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "\n",
    "        # Replay buffers to pass\n",
    "        self.o_buffer = np.zeros((self.ep_len_rollout, self.odim))\n",
    "        self.a_buffer = np.zeros((self.ep_len_rollout, self.adim))\n",
    "        self.r_buffer = np.zeros((self.ep_len_rollout))\n",
    "        self.v_t_buffer = np.zeros((self.ep_len_rollout))\n",
    "        self.logp_t_buffer = np.zeros((self.ep_len_rollout))\n",
    "\n",
    "        # Create PPO model\n",
    "        # Actor-critic model\n",
    "        ac_kwargs = dict()\n",
    "        ac_kwargs['action_space'] = self.env.action_space\n",
    "        self.model = ActorCritic(odim, adim, hdims, **ac_kwargs)\n",
    "        # Buffer\n",
    "        self.buf = PPOBuffer(odim=self.odim, adim=self.adim,\n",
    "                             size=self.ep_len_rollout, gamma=gamma, lam=lam)\n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "\n",
    "    def set_weights(self, weight_vals):\n",
    "        return self.model.load_state_dict(weight_vals)\n",
    "\n",
    "    def rollout(self):\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset()  # reset environment\n",
    "        # Loop\n",
    "        for t in range(self.ep_len_rollout):\n",
    "            a, _, logp_t, v_t, _ = self.model(torch.Tensor(self.o.reshape(1, -1)))  # pi, logp, logp_pi, v, mu\n",
    "            o2, r, d, _ = self.env.step(a.detach().numpy()[0])\n",
    "            # save and log\n",
    "            self.buf.store(self.o, a, r, v_t, logp_t)\n",
    "            # Update obs (critical!)\n",
    "            self.o = o2\n",
    "            if d:\n",
    "                self.buf.finish_path(last_val=0.0)\n",
    "                self.o = self.env.reset()  # reset when done\n",
    "        last_val = self.model.vf_mlp(torch.Tensor(self.o.reshape(1, -1))).item()\n",
    "        self.buf.finish_path(last_val)\n",
    "        return self.buf.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "hdims = [32, 32]\n",
    "#Graph\n",
    "clip_ratio = 0.2\n",
    "pi_lr = 3e-4\n",
    "vf_lr = 1e-3\n",
    "epsilon = 1e-2\n",
    "#Buffer\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "#Update\n",
    "train_pi_iters = 100\n",
    "train_v_iters = 100\n",
    "target_kl = 0.01\n",
    "epochs = 1000\n",
    "max_ep_len = 1000\n",
    "#Worker\n",
    "n_cpu = n_workers = 2\n",
    "total_steps = 1000\n",
    "evaluate_every = 50\n",
    "print_every = 10\n",
    "ep_len_rollout = 500\n",
    "batch_size = 4096\n",
    "\n",
    "RENDER_ON_EVAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def save_plot_data(plot_dict,file_name):\n",
    "    # save dictionary to json\n",
    "    with open('plot_data/%s.json'%file_name,'wb') as fp:\n",
    "        pickle.dump(plot_dict, fp)\n",
    "    return print(\"plot_data saved!\")\n",
    "\n",
    "def draw_subplt_graph(file_name1, file_name2):\n",
    "    # load json file\n",
    "    with open('plot_data/%s.json'%file_name1,'rb') as fp:\n",
    "        plot_dict1 = pickle.load(fp)\n",
    "    with open('plot_data/%s.json'%file_name2,'rb') as fp2:\n",
    "        plot_dict2 = pickle.load(fp2)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    # plt.subplots_adjust(hspace=0.5)\n",
    "    # make ep_ret-total step graph\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(list(plot_dict1.keys()),plot_dict1.values(),marker='o')\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('ep_return')\n",
    "    plt.title(\"hdim:%s gamma:[%.4f] lam:[%.4f] clip_ratio:[%.4f]\\n epsilon:[%.4f] ep_len_rollout:[%d]\\n\"\n",
    "              %(hdims,gamma,lam,clip_ratio,epsilon,ep_len_rollout))\n",
    "    plt.grid(True, linestyle='--')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(list(plot_dict2.keys()),plot_dict2.values(),marker='o')\n",
    "    plt.xlabel('time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('ep_return')\n",
    "    # plt.title(\"hdim:%s gamma:[%.4f] lam:[%.4f] clip_ratio:[%.4f]\\n epsilon:[%.4f] ep_len_rollout:[%d]\"\n",
    "    #           %(hdims,gamma,lam,clip_ratio,epsilon,ep_len_rollout))\n",
    "    plt.grid(True, linestyle='--')\n",
    "\n",
    "    plt.savefig('plot_data/plot_images/%s.png'%file_name1,dpi=100)\n",
    "    #plt.show()\n",
    "\n",
    "def draw_graph(file_name):\n",
    "    # load json file\n",
    "    with open('plot_data/%s.json'%file_name,'rb') as fp:\n",
    "        plot_dict = pickle.load(fp)\n",
    "\n",
    "    # make ep_ret-total step graph\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(list(plot_dict.keys()),plot_dict.values(),marker='o')\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('ep_return')\n",
    "    plt.title(\"hdim:%s gamma:[%.4f] lam:[%.4f] clip_ratio:[%.4f]\\n epsilon:[%.4f] ep_len_rollout:[%d]\"\n",
    "              %(hdims,gamma,lam,clip_ratio,epsilon,ep_len_rollout))\n",
    "    plt.grid(True, linestyle='--')\n",
    "\n",
    "    plt.savefig('plot_data/plot_images/%s.png'%file_name,dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Pytorch version:[%s].\"%(torch.__version__))\n",
    "\n",
    "    # Initialize PyBullet Ant Environment\n",
    "    eval_env = get_eval_env()\n",
    "    adim,odim = eval_env.action_space.shape[0],eval_env.observation_space.shape[0]\n",
    "    print(\"Environment Ready. odim:[%d] adim:[%d].\"%(odim,adim))\n",
    "\n",
    "    # Initialize Workers\n",
    "    ray.init(num_cpus=n_cpu)\n",
    "\n",
    "    R = RolloutWorkerClass(seed=0)\n",
    "    workers = [RayRolloutWorkerClass.remote(worker_id=i,ep_len_rollout=ep_len_rollout)\n",
    "            for i in range(int(n_workers))]\n",
    "    print(\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "        (n_cpu, n_workers))\n",
    "    time.sleep(1)\n",
    "    # Loop\n",
    "    start_time = time.time()\n",
    "    n_env_step = 0  # number of environment steps\n",
    "    plot_dict = {} # for visualization\n",
    "    plot_dict_time = {} # for visualization\n",
    "    for t in range(int(total_steps)):\n",
    "        esec = time.time() - start_time\n",
    "        # 1. Synchronize worker weights\n",
    "        weights = R.get_weights()\n",
    "        set_weights_list = [worker.set_weights.remote(weights) for worker in workers]\n",
    "        # 2. Make rollout and accumulate to Buffers\n",
    "        t_start = time.time()\n",
    "        ops = [worker.rollout.remote() for worker in workers]\n",
    "        rollout_vals = ray.get(ops)\n",
    "        sec_rollout = time.time() - t_start\n",
    "        # 3. Update\n",
    "        t_start = time.time()  # tic\n",
    "        # Mini-batch type of update\n",
    "        for r_idx, rval in enumerate(rollout_vals):\n",
    "            obs_buf, act_buf, adv_buf, ret_buf, logp_buf = \\\n",
    "                rval[0], rval[1], rval[2], rval[3], rval[4]\n",
    "            if r_idx == 0:\n",
    "                obs_bufs, act_bufs, adv_bufs, ret_bufs, logp_bufs = \\\n",
    "                    obs_buf, act_buf, adv_buf, ret_buf, logp_buf\n",
    "            else:\n",
    "                obs_bufs = np.concatenate((obs_bufs, obs_buf), axis=0)\n",
    "                act_bufs = np.concatenate((act_bufs, act_buf), axis=0)\n",
    "                adv_bufs = np.concatenate((adv_bufs, adv_buf), axis=0)\n",
    "                ret_bufs = np.concatenate((ret_bufs, ret_buf), axis=0)\n",
    "                logp_bufs = np.concatenate((logp_bufs, logp_buf), axis=0)\n",
    "        n_val_total = obs_bufs.shape[0]\n",
    "        for pi_iter in range(int(train_pi_iters)):\n",
    "            rand_idx = np.random.permutation(n_val_total)[:batch_size]\n",
    "            buf_batches = [obs_bufs[rand_idx], act_bufs[rand_idx], adv_bufs[rand_idx],\n",
    "                        ret_bufs[rand_idx], logp_bufs[rand_idx]]\n",
    "            obs, act, adv, ret, logp_a_old = [torch.Tensor(x) for x in buf_batches]\n",
    "            ent = (-logp_a_old).mean()\n",
    "            _, logp_a, _, _ = R.model.policy(obs, act)\n",
    "            # PPO objectives\n",
    "            ratio = (logp_a - logp_a_old).exp()\n",
    "            min_adv = torch.where(adv > 0, (1 + clip_ratio) * adv, (1 - clip_ratio) * adv)\n",
    "            pi_loss = -(torch.min(ratio * adv, min_adv)).mean()\n",
    "            R.train_pi.zero_grad(set_to_none=True)\n",
    "            pi_loss.backward()\n",
    "            R.train_pi.step()\n",
    "            # a sample estimate for KL-divergence\n",
    "            kl = torch.mean(logp_a_old - logp_a)\n",
    "            if kl > 1.5 * target_kl:\n",
    "                #print(\"  pi_iter:[%d] kl(%.3f) is higher than 1.5x(%.3f)\" % (pi_iter, kl, target_kl))\n",
    "                break\n",
    "        # Value gradient step\n",
    "        for _ in range(int(train_v_iters)):\n",
    "            rand_idx = np.random.permutation(n_val_total)[:batch_size]\n",
    "            buf_batches = [obs_bufs[rand_idx], act_bufs[rand_idx], adv_bufs[rand_idx],\n",
    "                        ret_bufs[rand_idx], logp_bufs[rand_idx]]\n",
    "            obs, act, adv, ret, logp = [torch.Tensor(x) for x in buf_batches]\n",
    "            v = R.model.vf_mlp(obs).squeeze()\n",
    "            v_loss = F.mse_loss(v, ret)\n",
    "            R.train_v.zero_grad(set_to_none=True)\n",
    "            v_loss.backward()\n",
    "            R.train_v.step()\n",
    "        sec_update = time.time() - t_start  # toc\n",
    "        # Print\n",
    "        if (t == 0) or (((t + 1) % print_every) == 0):\n",
    "            print(\"[%d/%d] rollout:[%.1f]s pi_iter:[%d/%d] update:[%.1f]s kl:[%.4f] target_kl:[%.4f].\" %\n",
    "                (t + 1, total_steps, sec_rollout, pi_iter, train_pi_iters, sec_update, kl, target_kl))\n",
    "            print(\"   pi_loss:[%.4f], entropy:[%.4f]\"%(pi_loss, ent))\n",
    "\n",
    "        # Evaluate\n",
    "        if (t == 0) or (((t + 1) % evaluate_every) == 0):\n",
    "            ram_percent = psutil.virtual_memory().percent  # memory usage\n",
    "            times = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))\n",
    "            print(\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\" %\n",
    "                (t + 1, total_steps, t / total_steps * 100,\n",
    "                n_env_step,times,ram_percent)\n",
    "                )\n",
    "            plot_dict[t] = 0    #for visualization\n",
    "            plot_dict_time[times] = 0   #for visualization\n",
    "            o, d, ep_ret, ep_len = eval_env.reset(), False, 0, 0\n",
    "            if RENDER_ON_EVAL:\n",
    "                _ = eval_env.render(mode='human')\n",
    "\n",
    "            while not (d or (ep_len == max_ep_len)):\n",
    "                a, _, _, _ = R.model.policy(torch.Tensor(o.reshape(1, -1)))\n",
    "                o, r, d, _ = eval_env.step(a.detach().numpy()[0])\n",
    "                if RENDER_ON_EVAL:\n",
    "                    _ = eval_env.render(mode='human')\n",
    "                ep_ret += r  # compute return\n",
    "                ep_len += 1\n",
    "            print(\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\" % (ep_ret, ep_len))\n",
    "            plot_dict[t] = ep_ret #for visualization\n",
    "            plot_dict_time[times] = ep_ret #for visualization\n",
    "\n",
    "    # for visualization\n",
    "    file_name = 'PPO_plt_data_2'\n",
    "    file_name2 = 'PPO_plt_data_time_2'\n",
    "\n",
    "    save_plot_data(plot_dict=plot_dict,file_name=file_name)\n",
    "    save_plot_data(plot_dict=plot_dict_time,file_name=file_name2)\n",
    "    draw_subplt_graph(file_name1=file_name,file_name2=file_name2)\n",
    "\n",
    "    # Close\n",
    "    print(\"Done.\")\n",
    "    eval_env.close()\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a336408ed5cf483d0b6697114c3c65340b20f5cdc6d74b72c0de63404a47289a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
