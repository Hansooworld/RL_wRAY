{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries & Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, gym, os, pybullet_envs, psutil, time, os\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "import datetime,gym,os,pybullet_envs,time,psutil,ray\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from gym.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_pi = 0.1\n",
    "alpha_q = 0.1 #0.1\n",
    "\n",
    "lr = 3e-4\n",
    "gamma = 0.98\n",
    "polyak = 0.995\n",
    "epsilon = 1e-2\n",
    "\n",
    "hdims = [256,256]\n",
    "\n",
    "n_cpu = n_workers = 8\n",
    "total_steps = 1e6\n",
    "start_steps = 1e4\n",
    "evaluate_every = 1e4\n",
    "ep_len_rollout = 100\n",
    "batch_size = 128\n",
    "update_count = 2\n",
    "num_eval = 3\n",
    "max_ep_len_eval = 1e3\n",
    "buffer_size_long = 1e6\n",
    "buffer_size_short = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(odim=24, hdims=[256, 256], actv='relu', output_actv='relu'):\n",
    "    ki = tf.keras.initializers.truncated_normal(stddev=0.1)\n",
    "    layers = tf.keras.Sequential()\n",
    "    layers.add(tf.keras.layers.InputLayer(input_shape=(odim,)))\n",
    "    for hdim in hdims[:-1]:\n",
    "        layers.add(tf.keras.layers.Dense(hdim, activation=actv, kernel_initializer=ki))\n",
    "    layers.add(tf.keras.layers.Dense(hdims[-1], activation=output_actv, kernel_initializer=ki))\n",
    "    return layers\n",
    "\n",
    "def gaussian_loglik(x,mu,log_std):\n",
    "        EPS = 1e-8\n",
    "        pre_sum = -0.5*(\n",
    "            ( (x-mu)/(tf.exp(log_std)+EPS) )**2 +\n",
    "            2*log_std + np.log(2*np.pi)\n",
    "        )\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "\n",
    "class MLPGaussianPolicy(tf.keras.Model):    # def mlp_gaussian_policy\n",
    "    def __init__(self, odim, adim, args, actv='relu'):\n",
    "        super(MLPGaussianPolicy, self).__init__()\n",
    "        self.net = mlp(odim, args.hdims, actv, output_actv=actv) #feature\n",
    "        # mu layer\n",
    "        self.mu = tf.keras.layers.Dense(adim, activation=None)\n",
    "        # std layer\n",
    "        self.log_std = tf.keras.layers.Dense(adim, activation=None)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, o, get_logprob=True):\n",
    "        net_ouput = self.net(o)\n",
    "        mu = self.mu(net_ouput)\n",
    "        log_std = self.log_std(net_ouput)\n",
    "\n",
    "        LOG_STD_MIN, LOG_STD_MAX = -10.0, +2.0\n",
    "        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX) #log_std\n",
    "        std = tf.exp(log_std) # std\n",
    "\n",
    "        # Pre-squash distribution and sample\n",
    "        dist = tfp.distributions.Normal(mu, std)\n",
    "        pi = dist.sample()    # sampled\n",
    "\n",
    "        if get_logprob:\n",
    "            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "            # NOTE: The correction formula is a little bit magic. To get an understanding\n",
    "            # of where it comes from, check out the original SAC paper (arXiv 1801.01290)\n",
    "            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "            # Try deriving it yourself as a (very difficult) exercise. :)\n",
    "            # logp_pi = tf.reduce_sum(dist.log_prob(pi), axis=1) #gaussian log_likelihood # modified axis\n",
    "            logp_pi = gaussian_loglik(x=pi, mu=mu, log_std=log_std)\n",
    "            logp_pi -= tf.reduce_sum(2 * (np.log(2) - pi - tf.nn.softplus(-2 * pi)), axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "        mu, pi = tf.tanh(mu), tf.tanh(pi)\n",
    "        return mu, pi, logp_pi\n",
    "\n",
    "\n",
    "# Q-function mlp\n",
    "class MLPQFunction(tf.keras.Model):\n",
    "    def __init__(self, odim, adim, args, actv='relu'):\n",
    "        super().__init__()\n",
    "        self.q = mlp(odim+adim, hdims=args.hdims+[1], actv=actv, output_actv=None)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, o, a):\n",
    "        x = tf.concat([o, a], -1)\n",
    "        q = self.q(x)\n",
    "        return tf.squeeze(q, axis=1)   #Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(tf.keras.Model):   # def mlp_actor_critic\n",
    "    def __init__(self, odim, adim, args, actv='relu'):\n",
    "        super(MLPActorCritic,self).__init__()\n",
    "        self.alpha_pi = args.alpha_pi\n",
    "        self.alpha_q = args.alpha_q\n",
    "        self.gamma = args.gamma\n",
    "        self.policy = MLPGaussianPolicy(odim=odim, adim=adim, args=args, actv=actv)\n",
    "        self.q1 = MLPQFunction(odim=odim, adim=adim, args=args, actv=actv)\n",
    "        self.q2 = MLPQFunction(odim=odim, adim=adim, args=args, actv=actv)\n",
    "        # Optimizers\n",
    "        self.train_pi = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "        self.train_q1 = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "        self.train_q2 = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, o, deterministic=False):\n",
    "        mu, pi, _ = self.policy(o, False)\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        else:\n",
    "            return pi\n",
    "\n",
    "    @tf.function\n",
    "    def update_policy(self, data):\n",
    "        o = data['obs1']\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # pi losses\n",
    "            _, pi, logp_pi = self.policy(o)\n",
    "            q1_pi = self.q1(o, pi)\n",
    "            q2_pi = self.q2(o, pi)\n",
    "            min_q_pi = tf.minimum(q1_pi, q2_pi)\n",
    "            pi_loss = tf.reduce_mean(self.alpha_pi*logp_pi - min_q_pi)\n",
    "        variables = self.policy.trainable_variables\n",
    "        # grads = tape.gradient(pi_loss, variables)\n",
    "        # self.train_pi.apply_gradients(zip(grads, variables))\n",
    "        self.train_pi.minimize(pi_loss, variables, tape=tape)\n",
    "        # tf.print('pi_loss', pi_loss)\n",
    "        return pi_loss, logp_pi, min_q_pi\n",
    "\n",
    "    @tf.function\n",
    "    def update_Q(self, target, data):\n",
    "        o, a, r, o2, d = data['obs1'], data['acts'], data['rews'], data['obs2'], data['done']\n",
    "        # get target action from current policy\n",
    "        _, pi_next, logp_pi_next = self.policy(o2)\n",
    "        # Target value\n",
    "        q1_targ = target.q1(o2, pi_next)\n",
    "        q2_targ = target.q2(o2, pi_next)\n",
    "        min_q_targ = tf.minimum(q1_targ, q2_targ)\n",
    "        # Entropy-regularized Bellman backup\n",
    "        q_backup = tf.stop_gradient(r + self.gamma * (1 - d) * (min_q_targ - self.alpha_q * logp_pi_next))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q1 = self.q1(o, a)\n",
    "            q2 = self.q2(o, a)\n",
    "            # value(q) loss\n",
    "            q1_loss = 0.5*tf.losses.mse(q1,q_backup)\n",
    "            q2_loss = 0.5*tf.losses.mse(q2,q_backup)\n",
    "            value_loss = q1_loss + q2_loss\n",
    "\n",
    "        # grads1 = tape.gradient(q1_loss, self.q1.trainable_variables)\n",
    "        # self.train_q1.apply_gradients(zip(grads1, self.q1.trainable_variables))\n",
    "\n",
    "        self.train_q1.minimize(value_loss, self.q1.trainable_variables+self.q2.trainable_variables, tape=tape)\n",
    "\n",
    "        # grads2 = tape.gradient(q2_loss, self.q2.trainable_variables)\n",
    "        # self.train_q2.apply_gradients(zip(grads2, self.q2.trainable_variables))\n",
    "        # tf.print('q1', q1)\n",
    "        # tf.print('q2', q2)\n",
    "        # tf.print('value_loss', value_loss)\n",
    "        return value_loss, q1, q2, logp_pi_next, q_backup, q1_targ, q2_targ\n",
    "\n",
    "    @tf.function\n",
    "    def update_draft(self, target, data):\n",
    "        o, a, r, o2, d = data['obs1'], data['acts'], data['rews'], data['obs2'], data['done']\n",
    "        # get target action from current policy\n",
    "        _, pi_next, logp_pi_next = self.policy(o2)\n",
    "        # Target value\n",
    "        q1_targ = target.q1(o2, pi_next)\n",
    "        q2_targ = target.q2(o2, pi_next)\n",
    "        min_q_targ = tf.minimum(q1_targ, q2_targ)\n",
    "        # Entropy-regularized Bellman backup\n",
    "        q_backup = tf.stop_gradient(r + self.gamma * (1 - d) * (min_q_targ - self.alpha_q * logp_pi_next))\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            _, pi, logp_pi = self.policy(o)\n",
    "            q1 = self.q1(o, a)\n",
    "            q2 = self.q2(o, a)\n",
    "            # value(q) loss\n",
    "            q1_pi = tf.stop_gradient(self.q1(o, pi))\n",
    "            q2_pi = tf.stop_gradient(self.q2(o, pi))\n",
    "            min_q_pi = tf.minimum(q1_pi, q2_pi)\n",
    "            pi_loss = tf.reduce_mean(self.alpha_pi*logp_pi - min_q_pi)\n",
    "            q1_loss = 0.5*tf.losses.mse(q1,q_backup)\n",
    "            q2_loss = 0.5*tf.losses.mse(q2,q_backup)\n",
    "            value_loss = q1_loss + q2_loss\n",
    "            loss = value_loss + pi_loss\n",
    "\n",
    "        # self.train_pi.minimize(pi_loss, self.policy.trainable_variables, tape=tape)\n",
    "        # self.train_q1.minimize(value_loss, self.q1.trainable_variables+self.q2.trainable_variables, tape=tape)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.train_pi.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return pi_loss, value_loss, q1, q2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a336408ed5cf483d0b6697114c3c65340b20f5cdc6d74b72c0de63404a47289a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
