{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import pybulletgym\n",
    "\n",
    "print(\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor & Critic & PPO class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorClass(nn.Module):\n",
    "\n",
    "    def __init__(self, name='Actor', state_dim=4, action_dim=1, action_bound=1, learning_rate=1e-2, hdims=[128]):\n",
    "\n",
    "        # class initialize\n",
    "        super(ActorClass, self).__init__()\n",
    "\n",
    "        # ActorClass parameter initialize\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.hdims = hdims\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        # Dense Layer construction\n",
    "        prev_hdim = self.state_dim\n",
    "        for hdim in self.hdims:\n",
    "            self.layers.append(nn.Linear(prev_hdim, hdim, bias=True))\n",
    "            self.layers.append(nn.ReLU())  # activation function = relu\n",
    "            prev_hdim = hdim\n",
    "\n",
    "        # Concatenate all layers\n",
    "        self.net = nn.Sequential()\n",
    "        for l_idx, layer in enumerate(self.layers):\n",
    "            layer_name = \"%s_%02d\" % (type(layer).__name__.lower(), l_idx)\n",
    "            self.net.add_module(layer_name, layer)\n",
    "\n",
    "        # Final Layer (without activation)\n",
    "        self.net_mu = nn.Linear(prev_hdim, self.action_dim)\n",
    "        self.net_std = nn.Linear(prev_hdim, self.action_dim)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        mu_net = self.net_mu(x)\n",
    "        mu = self.action_bound * torch.tanh(mu_net)\n",
    "\n",
    "        std_net = self.net_std(x)\n",
    "        std = F.softplus(std_net)\n",
    "        return mu, std\n",
    "\n",
    "\n",
    "# Critic Layer Construction\n",
    "class CriticClass(nn.Module):\n",
    "    def __init__(self, name='Critic', state_dim=4, learning_rate=1e-2, hdims = [128]):\n",
    "\n",
    "        # class initialize\n",
    "        super(CriticClass, self).__init__()\n",
    "\n",
    "        # ActorClass parameter initialize\n",
    "        self.state_dim = state_dim\n",
    "        self.hdims = hdims\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        # Dense Layer construction\n",
    "        prev_hdim = self.state_dim\n",
    "        for hdim in self.hdims:\n",
    "            self.layers.append(nn.Linear(prev_hdim, hdim, bias=True))\n",
    "            self.layers.append(nn.ReLU())  # activation function = relu\n",
    "            prev_hdim = hdim\n",
    "\n",
    "        # Final Layer (without activation)\n",
    "        self.layers.append(nn.Linear(prev_hdim, 1))\n",
    "\n",
    "        # Concatenate all layers\n",
    "        self.net = nn.Sequential()\n",
    "        for l_idx, layer in enumerate(self.layers):\n",
    "            layer_name = \"%s_%02d\" % (type(layer).__name__.lower(), l_idx)\n",
    "            self.net.add_module(layer_name, layer)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        val = self.net(x)  # scalar\n",
    "        return val\n",
    "\n",
    "\n",
    "# PPO Class + train net\n",
    "class PPO:\n",
    "    def __init__(self, state_dim=4, action_dim=2, action_bound=1, lr_actor=1e-3, lr_critic=1e-2, eps_clip=0.2, K_epoch=3, gamma=0.98, lmbda=0.95, buffer_size=30, minibatch_size=32):\n",
    "\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epoch = K_epoch\n",
    "        self.lmbda = lmbda\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        # model\n",
    "        self.actor = ActorClass(state_dim=state_dim, action_dim=action_dim, action_bound=action_bound, learning_rate=lr_actor)\n",
    "        self.critic = CriticClass(state_dim=state_dim, learning_rate=lr_critic)\n",
    "        self.data = []\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "        for j in range(self.buffer_size):\n",
    "            for i in range(self.minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "\n",
    "                    s_lst.append(s)  # s:numpy array, shape:[...]\n",
    "                    a_lst.append(a)  # s와 shape을 맞추기 위해 []를 취함./ pytorch unsqueeze(차원 증가.)\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append(prob_a)\n",
    "                    if done:\n",
    "                        done_mask = 0\n",
    "                    else:\n",
    "                        done_mask = 1\n",
    "                    done_lst.append([done_mask])\n",
    "\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "\n",
    "            mini_batch = torch.tensor(np.array(s_batch), dtype=torch.float), torch.tensor(np.array(a_batch), dtype=torch.float),\\\n",
    "                         torch.tensor(np.array(r_batch), dtype=torch.float), torch.tensor(np.array(s_prime_batch), dtype=torch.float), \\\n",
    "                         torch.tensor(np.array(done_batch), dtype=torch.float), torch.tensor(np.array(prob_a_batch), dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "        return data\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def get_action(self, state, action_bound):\n",
    "        mu, std = self.actor.forward(torch.from_numpy(state).float())\n",
    "        dist = Normal(mu, std)\n",
    "        #action = torch.tanh(dist.sample())*action_bound\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        action = action.detach().numpy()\n",
    "        log_prob = np.sum(log_prob.detach().numpy())\n",
    "        return action, log_prob\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + self.gamma * self.critic.forward(s_prime) * done_mask\n",
    "                delta = td_target - self.critic.forward(s)\n",
    "            delta = delta.numpy()\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = self.gamma * self.lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "        return data_with_adv\n",
    "\n",
    "    def train_net(self):\n",
    "        if len(self.data) == self.minibatch_size * self.buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(self.K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "                    mu, std = self.actor.forward(s)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    log_prob = np.sum(log_prob.detach().numpy())\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.critic.forward(s), td_target)\n",
    "\n",
    "                    self.actor.optimizer.zero_grad()\n",
    "                    self.critic.optimizer.zero_grad()\n",
    "\n",
    "                    loss.mean().backward()\n",
    "                    nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "                    nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n",
    "\n",
    "                    self.actor.optimizer.step()\n",
    "                    self.critic.optimizer.step()\n",
    "\n",
    "                    self.actor.optimization_step += 1\n",
    "                    self.critic.optimization_step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ####### Hyperparameters #######\n",
    "    env = gym.make('AntPyBulletEnv-v0')       #'Pendulum-v0','AntPyBulletEnv-v0'\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    print(state_dim)\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    print(action_dim)\n",
    "    action_bound = env.action_space.high[0]\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    eps_clip = 0.2\n",
    "    K_epoch = 10\n",
    "    lmbda = 0.9\n",
    "    gamma = 0.9\n",
    "    learning_rate_actor = 0.001\n",
    "    learning_rate_critic = 0.001\n",
    "    rollout_len = 3\n",
    "    buffer_size = 30\n",
    "    minibatch_size = 32\n",
    "    ###############################\n",
    "\n",
    "    agent = PPO(state_dim=state_dim, action_dim=action_dim, action_bound=action_bound, lr_actor=learning_rate_actor, lr_critic=learning_rate_critic,\\\n",
    "                eps_clip=eps_clip, K_epoch=K_epoch, gamma=gamma, lmbda=lmbda, buffer_size=buffer_size, minibatch_size=minibatch_size)\n",
    "\n",
    "    rollout = []\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        #env.render()\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(rollout_len):\n",
    "                a, log_prob = agent.get_action(s, action_bound)\n",
    "                #env.render()\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "                rollout.append((s, a, r, s_prime, log_prob, done))\n",
    "                if len(rollout) == rollout_len:\n",
    "                    agent.put_data(rollout)\n",
    "                    rollout = []\n",
    "                s = s_prime\n",
    "                score += r\n",
    "                #env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            agent.train_net()\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, opt step: {}\".format(n_epi, score / print_interval, agent.actor.optimization_step))\n",
    "            score = 0.0\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a336408ed5cf483d0b6697114c3c65340b20f5cdc6d74b72c0de63404a47289a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
