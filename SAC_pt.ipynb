{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries & Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybulletgym\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch._C import Size\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for SAC Algorithm\n",
    "env = gym.make('AntPyBulletEnv-v0')\n",
    "buffer_limit = 1000000\n",
    "lr_q = 0.0003\n",
    "lr_pi = 0.0003\n",
    "lr_alpha = 0.0003\n",
    "gamma = 0.99\n",
    "batch_size = 256\n",
    "init_alpha = 0.1\n",
    "tau = 0.005\n",
    "target_entropy = -env.action_space.shape[0]\n",
    "\n",
    "# Params for run & save model\n",
    "folder =0 \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReplayBuffer & Actor & Critic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self,item):\n",
    "        self.buffer.append(item)\n",
    "\n",
    "    def sample(self,n):\n",
    "        mini_batch = random.sample(self.buffer,n)\n",
    "        s_list, a_list, r_list, s_prime_list, done_mask_list = [], [], [], [], []\n",
    "\n",
    "        for item in mini_batch:\n",
    "            s, a, r, s_prime, done = item\n",
    "            s_list.append(s)\n",
    "            a_list.append(a)\n",
    "            r_list.append([r])\n",
    "            s_prime_list.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_list.append([done_mask])\n",
    "        s_list = torch.tensor(np.array(s_list), dtype = torch.float).to(device)\n",
    "        a_list = torch.tensor(np.array(a_list), dtype = torch.float).to(device)\n",
    "        r_list = torch.tensor(np.array(r_list), dtype = torch.float).to(device)\n",
    "        s_prime_list = torch.tensor(np.array(s_prime_list), dtype = torch.float).to(device)\n",
    "        done_mask_list = torch.tensor(np.array(done_mask_list), dtype = torch.float).to(device)\n",
    "        return s_list, a_list, r_list, s_prime_list, done_mask_list\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(Actor,self).__init__()\n",
    "        # Gaussian Distribution\n",
    "        self.fc1 = nn.Linear(28,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc_mean = nn.Linear(256,8)\n",
    "        self.fc_std = nn.Linear(256,8)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "\n",
    "        # Autotuning Alpha\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha],lr = lr_alpha)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        # std : softplus or ReLU activate function\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        Gaussian = Normal(mean,std)\n",
    "        action = Gaussian.rsample()\n",
    "        log_prob = Gaussian.log_prob(action)\n",
    "        # action range : -1 ~ 1\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7)\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_p(self,q1,q2,mini_batch):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        loss = (-min_q - entropy)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "pi = Actor(lr_pi)\n",
    "print(\"Actor loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc_s = nn.Linear(28,128)\n",
    "        self.fc_a = nn.Linear(8,128)\n",
    "        self.fc_cat = nn.Linear(256,256)\n",
    "        self.fc_out = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "\n",
    "    def forward(self,x,a):\n",
    "        x = F.relu(self.fc_s(x))\n",
    "        a = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([x,a], dim=1)\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q_value = self.fc_out(q)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "    def train_q(self,target,mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s,a), target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # DDPG soft_update\n",
    "    def soft_update(self, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(pi, q1, q2, mini_batch):\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "    with torch.no_grad():\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q, 1, keepdim=True)[0]\n",
    "        target = r + gamma * done * (min_q + entropy.mean())\n",
    "    return target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to render pybulletgym & SAC alogirhtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ant_render():\n",
    "    env = gym.make('AntPyBulletEnv-v0')\n",
    "    s = env.reset()\n",
    "    global pi\n",
    "    score = 0\n",
    "    done = False\n",
    "    plt.figure(figsize=(9,9))\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    while done is not True:\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        a, _ = pi(torch.from_numpy(s).float())\n",
    "        a_ = []\n",
    "        for i in a:\n",
    "            a_.append(i.item())\n",
    "        s_prime, r, done, info = env.step(a_)\n",
    "        score += r\n",
    "        s = s_prime\n",
    "        \n",
    "        if done is True:\n",
    "            env.close()\n",
    "            break\n",
    "    print(\"Rendering is Finished\")\n",
    "    print(\"Final Ant Score : {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    memory = ReplayBuffer()\n",
    "    q1, q2, q1_target, q2_target = Critic(lr_q), Critic(lr_q), Critic(lr_q), Critic(lr_q)\n",
    "    global pi\n",
    "    q1_target.load_state_dict(q1.state_dict())\n",
    "    q2_target.load_state_dict(q2.state_dict())\n",
    "\n",
    "    score = 0\n",
    "    best_score = 0\n",
    "    print_interval = 100\n",
    "    gradient_update = 30\n",
    "    step = 0\n",
    "    # env.render()\n",
    "    for episodes in range(1,100000):\n",
    "        s = env.reset()\n",
    "        bestsc = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, log_prob = pi(torch.from_numpy(s).float())\n",
    "            a_ = []\n",
    "            for i in a:\n",
    "                a_.append(i.item())\n",
    "            s_prime, r, done, info = env.step(a_)\n",
    "            memory.put((s,a_,r,s_prime,done))\n",
    "            score += r\n",
    "            bestsc += r\n",
    "            step += 1\n",
    "            s = s_prime\n",
    "        if bestsc > best_score:\n",
    "            best_score = bestsc\n",
    "        \n",
    "        if memory.size() > 30000:\n",
    "            for i in range(gradient_update):\n",
    "                mini_batch = memory.sample(batch_size)\n",
    "                td_target = get_target(pi, q1_target, q2_target, mini_batch)\n",
    "                q1.train_q(td_target, mini_batch)\n",
    "                q2.train_q(td_target, mini_batch)\n",
    "                pi.train_p(q1, q2, mini_batch)\n",
    "                q1.soft_update(q1_target)\n",
    "                q2.soft_update(q2_target)\n",
    "            \n",
    "        if episodes % print_interval==0 and episodes!=0:\n",
    "            print(\"number of episode :{}, avg score :{:.1f}, best score :{:.1f}, avg step :{:.1f}, alpha:{:.4f}\".format(episodes, score/print_interval, best_score, step/print_interval, pi.log_alpha.exp()))\n",
    "            if not os.path.exists(\"weights_{}\".format(folder)):\n",
    "                os.mkdir(\"weights_{}\".format(folder))\n",
    "            torch.save(pi.state_dict(), 'weights_{}/model_weights_{}.pth'.format(folder,episodes))\n",
    "            score = 0\n",
    "            step = 0\n",
    "\n",
    "        if episodes >= 2500:\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop and render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_ant_render()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a336408ed5cf483d0b6697114c3c65340b20f5cdc6d74b72c0de63404a47289a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
