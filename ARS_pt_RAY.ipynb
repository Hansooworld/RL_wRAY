{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "##### Model construction #####\n",
    "class MLP(nn.Module):       # def mlp in def create_ppo_model\n",
    "    def __init__(self, o_dim=24, a_dim=8, hdims=[64,64], actv=nn.ReLU(),\n",
    "                 output_actv=nn.ReLU()):\n",
    "        super(MLP, self).__init__()\n",
    "        self.o_dim = o_dim\n",
    "        self.hdims = hdims\n",
    "        self.actv = actv\n",
    "        self.ouput_actv = output_actv\n",
    "        self.layers = []\n",
    "        prev_hdim = self.o_dim\n",
    "        for hdim in self.hdims:\n",
    "            linear = nn.Linear(prev_hdim, hdim)\n",
    "            nn.init.trunc_normal_(linear.weight, std=0.1)\n",
    "            self.layers.append(linear)\n",
    "            self.layers.append(actv)\n",
    "            prev_hdim = hdim\n",
    "        linear_out = nn.Linear(prev_hdim, a_dim)\n",
    "        nn.init.trunc_normal_(linear_out.weight, std=0.1)  # add!\n",
    "        self.layers.append(linear_out)\n",
    "        self.net = nn.Sequential()\n",
    "        for l_idx, layer in enumerate(self.layers):\n",
    "            layer_name = \"%s_%02d\" % (type(layer).__name__.lower(), l_idx)\n",
    "            self.net.add_module(layer_name, layer)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = obs\n",
    "        if self.ouput_actv is None:\n",
    "            mu = self.net(x)\n",
    "        else:\n",
    "            mu = self.net(x)\n",
    "            mu = self.actv(mu)\n",
    "        return mu\n",
    "\n",
    "# weight와 동일하게 dictionary형태로 noise 만들고 main에서 더하는 걸로.\n",
    "def get_noises_from_weights(weights, nu=0.01):\n",
    "    noises = {}\n",
    "    for key, value in weights.items():\n",
    "        noises[key] = nu * torch.randn(value.shape)     #randn 함수로 설정해야함. rand는 uniform random\n",
    "    return noises # dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ray Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "RENDER_ON_EVAL = False\n",
    "\n",
    "def get_env():\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger\n",
    "    env = gym.make('AntBulletEnv-v0')\n",
    "    return env\n",
    "\n",
    "def get_eval_env():\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger\n",
    "    eval_env = gym.make('AntBulletEnv-v0')\n",
    "    if RENDER_ON_EVAL:\n",
    "        _ = eval_env.render(mode='human') # enable rendering\n",
    "    _ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering\n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return eval_env\n",
    "\n",
    "class RolloutWorkerClass(object):\n",
    "    def __init__(self,hdims=[128], actv=nn.ReLU, out_actv=nn.Tanh,seed=1):\n",
    "        self.seed = seed\n",
    "        self.env = get_env()\n",
    "        odim, adim = self.env.observation_space.shape[0], self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "\n",
    "        #ARS model\n",
    "        self.model = MLP(o_dim=self.odim, a_dim=self.adim,\n",
    "                         hdims=hdims, actv=actv, output_actv=out_actv)\n",
    "        # # model load\n",
    "        # self.model.load_state_dict(torch.load('model_data/model_weights_210524'))\n",
    "        # print(\"weight load\")\n",
    "\n",
    "        # Initialize model\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def get_action(self, o):\n",
    "        return self.model(torch.Tensor(o.reshape(1, -1)))\n",
    "\n",
    "    def get_weights(self):\n",
    "        weight_vals = self.model.state_dict()\n",
    "        return weight_vals\n",
    "\n",
    "    def set_weights(self, weight_vals):\n",
    "        return self.model.load_state_dict(weight_vals)\n",
    "\n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    def __init__(self, worker_id=0, hdims=[128], actv=nn.ReLU,\n",
    "                 out_actv=nn.Tanh, ep_len_rollout=1000):\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        self.env = get_env()\n",
    "        odim, adim = self.env.observation_space.shape[0], self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        #ARS model\n",
    "        self.model = MLP(o_dim=self.odim, a_dim=self.adim,\n",
    "                         hdims=hdims, actv=actv, output_actv=out_actv)\n",
    "\n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "\n",
    "    def get_action(self, o):\n",
    "        return self.model(o)\n",
    "\n",
    "    def set_weights(self, weight_vals, noise, noise_sign):\n",
    "        weight_val_noise = {}\n",
    "        for key, value in weight_vals.items():\n",
    "            weight_val_noise[key] = weight_vals[key] + noise_sign*noise[key]\n",
    "        return self.model.load_state_dict(weight_val_noise)\n",
    "\n",
    "    def rollout(self):\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset()  # reset environment\n",
    "        # Loop\n",
    "        self.o = self.env.reset()  # reset always\n",
    "        r_sum, step = 0, 0\n",
    "        self.a = self.get_action(torch.Tensor(self.o.reshape(1, -1)))\n",
    "        for t in range(self.ep_len_rollout):\n",
    "            self.a = self.get_action(torch.Tensor(self.o.reshape(1, -1)))\n",
    "            self.o2, self.r, self.d, _ = self.env.step(self.a.detach().numpy()[0])\n",
    "            # Save next state\n",
    "            self.o = self.o2\n",
    "            # Accumulate reward\n",
    "            r_sum += self.r\n",
    "            step += 1\n",
    "            if self.d:\n",
    "                break\n",
    "        return r_sum, step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Configuration\n",
    "n_cpu = n_workers = 50 #100\n",
    "total_steps = 1000 #5000\n",
    "evaluate_every = 50\n",
    "print_every = 10\n",
    "ep_len_rollout = 1000\n",
    "num_eval = 3\n",
    "max_ep_len_eval = 1000\n",
    "n_env_step = 0\n",
    "hdims = [256,256]\n",
    "actv = nn.Tanh()\n",
    "out_actv = nn.Tanh()\n",
    "\n",
    "alpha = 0.01\n",
    "nu = 0.06   #0.03 \n",
    "b = (n_workers//5)      # 0.01,0.03,(n_workers//5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def save_plot_data(plot_dict,file_name):\n",
    "    # save dictionary to json\n",
    "    with open('plot_data/%s.json'%file_name,'wb') as fp:\n",
    "        pickle.dump(plot_dict, fp)\n",
    "    return print(\"plot_data saved!\")\n",
    "\n",
    "def draw_graph(file_name):\n",
    "    # load json file\n",
    "    with open('plot_data/%s.json'%file_name,'rb') as fp:\n",
    "        plot_dict = pickle.load(fp)\n",
    "\n",
    "    # make ep_ret-total step graph\n",
    "    plt.plot(list(plot_dict.keys()),plot_dict.values(),marker='o')\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('ep_return')\n",
    "    plt.title(\"hdim:%s alpha:[%.4f] nu:[%.4f] b:[%.4f] ep_len_rollout:[%d]\"\n",
    "              %(hdims,alpha,nu,b,ep_len_rollout))\n",
    "    plt.grid(True, linestyle='--')\n",
    "    plt.savefig('plot_data/plot_images/%s.png'%file_name,dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    eval_env = get_eval_env()\n",
    "    adim, odim = eval_env.action_space.shape[0], eval_env.observation_space.shape[0]\n",
    "    print(\"Environment Ready. odim:[%d] adim:[%d].\" % (odim, adim))\n",
    "\n",
    "    ray.init(num_cpus=n_cpu)\n",
    "    R = RolloutWorkerClass(hdims=hdims,actv=actv,out_actv=out_actv,seed=0)\n",
    "    workers = [RayRolloutWorkerClass.remote(worker_id=i,hdims=hdims,actv=actv,\n",
    "                                            out_actv=out_actv,ep_len_rollout=ep_len_rollout) for i in range(n_workers)]\n",
    "    print(\"RAY initialized with [%d] cpus and [%d] workers.\"%(n_cpu,n_workers))\n",
    "\n",
    "    start_time = time.time()\n",
    "    plot_dict = {} # for visualization\n",
    "    for t in range(int(total_steps)):\n",
    "        # Distribute worker weights\n",
    "        weights = R.get_weights()\n",
    "        noises_list = []\n",
    "        for _ in range(n_workers): #worker마다 noise값 다르게 가져간다.\n",
    "            noises_list.append(get_noises_from_weights(weights, nu=nu))\n",
    "\n",
    "        # Positive rollouts (noise_sign=+1)\n",
    "        set_weights_list = [worker.set_weights.remote(weights, noises, noise_sign=+1)\n",
    "                            for worker, noises in zip(workers, noises_list)]\n",
    "        ops = [worker.rollout.remote() for worker in workers]\n",
    "        res_pos = ray.get(ops)\n",
    "        rollout_pos_vals, r_idx = np.zeros(n_workers), 0\n",
    "        for rew, eplen in res_pos:\n",
    "            rollout_pos_vals[r_idx] = rew\n",
    "            r_idx = r_idx + 1\n",
    "            n_env_step += eplen\n",
    "\n",
    "        # Negative rollouts (noise_sign=-1)\n",
    "        set_weights_list = [worker.set_weights.remote(weights, noises, noise_sign=-1)\n",
    "                            for worker, noises in zip(workers, noises_list)]\n",
    "        ops = [worker.rollout.remote() for worker in workers]\n",
    "        res_neg = ray.get(ops)\n",
    "        rollout_neg_vals, r_idx = np.zeros(n_workers), 0\n",
    "        for rew, eplen in res_neg:\n",
    "            rollout_neg_vals[r_idx] = rew\n",
    "            r_idx = r_idx + 1\n",
    "            n_env_step += eplen\n",
    "\n",
    "        # Scale reward\n",
    "        rollout_pos_vals, rollout_neg_vals = rollout_pos_vals / 100, rollout_neg_vals / 100\n",
    "\n",
    "        # Reward\n",
    "        rollout_concat_vals = np.concatenate((rollout_pos_vals, rollout_neg_vals))\n",
    "        rollout_delta_vals = rollout_pos_vals - rollout_neg_vals  # pos-neg\n",
    "        rollout_max_vals = np.maximum(rollout_pos_vals, rollout_neg_vals)\n",
    "        rollout_max_val = np.max(rollout_max_vals)  # single maximum\n",
    "        rollout_delta_max_val = np.max(np.abs(rollout_delta_vals))\n",
    "\n",
    "        # Sort\n",
    "        # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
    "        sort_idx = np.argsort(-rollout_max_vals)\n",
    "\n",
    "        # Update\n",
    "        sigma_R = np.std(rollout_concat_vals)   # Compute the standard deviation of all reward\n",
    "        weights_updated = {}\n",
    "        for key, weight in weights.items():  # for each weight\n",
    "            delta_weight_sum = np.zeros_like(weight)\n",
    "            for k in range(b):\n",
    "                idx_k = sort_idx[k]  # sorted index\n",
    "                rollout_delta_k = rollout_delta_vals[idx_k]\n",
    "                noises_k = noises_list[idx_k]\n",
    "                noise_k = (1 / nu) * noises_k[key]  # noise for current weight\n",
    "                delta_weight_sum += rollout_delta_k * noise_k.detach().numpy()\n",
    "            delta_weight = (alpha / (b * sigma_R)) * delta_weight_sum\n",
    "            weight = weight + delta_weight\n",
    "            weights_updated[key] = weight\n",
    "        # Set weight\n",
    "        R.set_weights(weights_updated)\n",
    "\n",
    "        # Print\n",
    "        if (t == 0) or (((t + 1) % print_every) == 0):\n",
    "            print(\"[%d/%d] rollout_max_val:[%.2f] rollout_delta_max_val:[%.2f] sigma_R:[%.2f] \" %\n",
    "                (t, total_steps, rollout_max_val, rollout_delta_max_val, sigma_R))\n",
    "\n",
    "        # Evaluate\n",
    "        if (t == 0) or (((t + 1) % evaluate_every) == 0) or (t == (total_steps - 1)):\n",
    "            ram_percent = psutil.virtual_memory().percent  # memory usage\n",
    "            print(\"[Evaluate] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\" %\n",
    "                (t + 1, total_steps, t / total_steps * 100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)),\n",
    "                ram_percent)\n",
    "                )\n",
    "            plot_dict[n_env_step] = 0\n",
    "            ep_ret_list = []  #for visualization\n",
    "            for eval_idx in range(num_eval):\n",
    "                o, d, ep_ret, ep_len = eval_env.reset(), False, 0, 0\n",
    "                if RENDER_ON_EVAL:\n",
    "                    _ = eval_env.render(mode='human')\n",
    "                while not (d or (ep_len == max_ep_len_eval)):\n",
    "                    a = R.get_action(o)\n",
    "                    o, r, d, _ = eval_env.step(a.detach().numpy()[0])\n",
    "                    if RENDER_ON_EVAL:\n",
    "                        _ = eval_env.render(mode='human')\n",
    "                    ep_ret += r  # compute return\n",
    "                    ep_len += 1\n",
    "                    ep_ret_list.append(ep_ret)  #for visualization\n",
    "                print(\" [Evaluate] [%d/%d] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "                    % (eval_idx, num_eval, ep_ret, ep_len))\n",
    "            plot_dict[n_env_step] = max(ep_ret_list)\n",
    "\n",
    "    # for visualization\n",
    "    file_name = 'ARS_plt_data_1'\n",
    "    save_plot_data(plot_dict=plot_dict,file_name=file_name)\n",
    "    draw_graph(file_name=file_name)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    eval_env.close()\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a336408ed5cf483d0b6697114c3c65340b20f5cdc6d74b72c0de63404a47289a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
