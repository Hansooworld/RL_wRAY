{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries & Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybulletgym\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch._C import Size\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import ray\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for SAC Algorithm\n",
    "env = gym.make('AntPyBulletEnv-v0')\n",
    "buffer_limit = 1000000\n",
    "lr_q = 0.0003\n",
    "lr_pi = 0.0003\n",
    "lr_alpha = 0.0003\n",
    "gamma = 0.99\n",
    "batch_size = 256\n",
    "init_alpha = 0.1\n",
    "tau = 0.005\n",
    "target_entropy = -env.action_space.shape[0]\n",
    "n_workers = 6\n",
    "n_cpu = 6\n",
    "gradient_step = 1 * n_workers\n",
    "\n",
    "# Params for run & save model\n",
    "folder =0 \n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReplayBuffer & Actor & Critic Class & RAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self,item):\n",
    "        self.buffer.append(item)\n",
    "\n",
    "    def sample(self,n):\n",
    "        mini_batch = random.sample(self.buffer,n)\n",
    "        s_list, a_list, r_list, s_prime_list, done_mask_list = [], [], [], [], []\n",
    "\n",
    "        for item in mini_batch:\n",
    "            s, a, r, s_prime, done = item\n",
    "            s_list.append(s)\n",
    "            a_list.append(a)\n",
    "            r_list.append([r])\n",
    "            s_prime_list.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_list.append([done_mask])\n",
    "        s_list = torch.tensor(np.array(s_list), dtype = torch.float).to(device)\n",
    "        a_list = torch.tensor(np.array(a_list), dtype = torch.float).to(device)\n",
    "        r_list = torch.tensor(np.array(r_list), dtype = torch.float).to(device)\n",
    "        s_prime_list = torch.tensor(np.array(s_prime_list), dtype = torch.float).to(device)\n",
    "        done_mask_list = torch.tensor(np.array(done_mask_list), dtype = torch.float).to(device)\n",
    "        return s_list, a_list, r_list, s_prime_list, done_mask_list\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(Actor,self).__init__()\n",
    "        # Gaussian Distribution\n",
    "        self.fc1 = nn.Linear(28,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc_mean = nn.Linear(256,8)\n",
    "        self.fc_std = nn.Linear(256,8)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "\n",
    "        # Autotuning Alpha\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha],lr = lr_alpha)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        # std : softplus or ReLU activate function\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        Gaussian = Normal(mean,std)\n",
    "        action = Gaussian.rsample()\n",
    "        log_prob = Gaussian.log_prob(action)\n",
    "        # action range : -1 ~ 1\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7)\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_p(self,q1,q2,mini_batch):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        loss = (-min_q - entropy)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "pi = Actor(lr_pi)\n",
    "print('Actor Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc_s = nn.Linear(28,128)\n",
    "        self.fc_a = nn.Linear(8,128)\n",
    "        self.fc_cat = nn.Linear(256,256)\n",
    "        self.fc_out = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "\n",
    "    def forward(self,x,a):\n",
    "        x = F.relu(self.fc_s(x))\n",
    "        a = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([x,a], dim=1)\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q_value = self.fc_out(q)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "    def train_q(self,target,mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s,a), target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # DDPG soft_update\n",
    "    def soft_update(self, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(pi, q1, q2, mini_batch):\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "    with torch.no_grad():\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q, 1, keepdim=True)[0]\n",
    "        target = r + gamma * done * (min_q + entropy.mean())\n",
    "    return target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self, device, seed=2000):\n",
    "        self.seed = seed\n",
    "        self.env = env\n",
    "\n",
    "        # Create SAC model and target networks\n",
    "        self.q1, self.q2, self.q1_target, self.q2_target = Critic(lr_q).to(device), Critic(lr_q).to(device), Critic(lr_q).to(device), Critic(lr_q).to(device)\n",
    "        self.pi = Actor(lr_pi).to(device)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        # Initialize model\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.q1.state_dict(), self.q2.state_dict(), self.q1_target.state_dict(), self.q2_target.state_dict()\n",
    "\n",
    "    def set_weights(self, q1_weight, q2_weight, q1_target_weight, q2_target_weight):\n",
    "        self.q1.load_state_dict(q1_weight)\n",
    "        self.q2.load_state_dict(q2_weight)\n",
    "        self.q1_target.load_state_dict(q1_target_weight)\n",
    "        self.q2_target.load_state_dict(q2_target_weight)\n",
    "\n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker with RAY (for rollout)\n",
    "    \"\"\"\n",
    "    def __init__(self, device, worker_id=0, seed=1):\n",
    "        self.seed = seed\n",
    "        self.worker_id = worker_id\n",
    "        self.env = env\n",
    "\n",
    "        # Create SAC model and target networks\n",
    "        self.q1, self.q2, self.q1_target, self.q2_target = Critic(lr_q).to(device), Critic(lr_q).to(device), Critic(lr_q).to(device), Critic(lr_q).to(device)\n",
    "        self.pi = Actor(lr_pi).to(device)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        # Initialize model\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def set_weights(self, q1_weight, q2_weight, q1_target_weight, q2_target_weight):\n",
    "        self.q1.load_state_dict(q1_weight)\n",
    "        self.q2.load_state_dict(q2_weight)\n",
    "        self.q1_target.load_state_dict(q1_target_weight)\n",
    "        self.q2_target.load_state_dict(q2_target_weight)\n",
    "\n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "    \n",
    "    def rollout(self, memory, rollout):\n",
    "        s = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        bestsc = 0\n",
    "        step = 0\n",
    "        small_buffer = collections.deque(maxlen=1000)\n",
    "\n",
    "        for i in range(1000):\n",
    "            a, log_prob = self.pi(torch.from_numpy(s).float().to(device))\n",
    "            a_ = []\n",
    "            # self.env.render()\n",
    "            for i in a:\n",
    "                a_.append(i.item())\n",
    "            s_prime, reward, done, info = self.env.step(a_)\n",
    "            small_buffer.append((s,a_,reward,s_prime,done))\n",
    "            score += reward\n",
    "            step += 1\n",
    "            s = s_prime \n",
    "    \n",
    "            if done is True:\n",
    "                break\n",
    "\n",
    "        return score, step, small_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to render pybulletgym & SAC alogirhtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ant_render():\n",
    "    env = gym.make('AntPyBulletEnv-v0')\n",
    "    s = env.reset()\n",
    "    global pi\n",
    "    score = 0\n",
    "    done = False\n",
    "    plt.figure(figsize=(9,9))\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    while done is not True:\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        a, _ = pi(torch.from_numpy(s).float())\n",
    "        a_ = []\n",
    "        for i in a:\n",
    "            a_.append(i.item())\n",
    "        s_prime, r, done, info = env.step(a_)\n",
    "        score += r\n",
    "        s = s_prime\n",
    "        \n",
    "        if done is True:\n",
    "            env.close()\n",
    "            break\n",
    "    print(\"Rendering is Finished\")\n",
    "    print(\"Final Ant Score : {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # For RAY\n",
    "    ray.init(num_cpus=n_cpu,\n",
    "         _memory = 5*1024*1024*1024,\n",
    "         object_store_memory = 10*1024*1024*1024,\n",
    "         _driver_object_store_memory = 1*1024*1024*1024)\n",
    "\n",
    "    R = RolloutWorkerClass(device, seed=0)\n",
    "    workers = [RayRolloutWorkerClass.remote(device, worker_id=i)\n",
    "           for i in range(n_workers)]\n",
    "    print(\"RAY initialized with [%d] cpus and [%d] workers.\"%(n_cpu,n_workers))\n",
    "\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    score = 0\n",
    "    bestsc = 0\n",
    "    print_interval = 100\n",
    "    gradient_update = 30\n",
    "    step = 0\n",
    "\n",
    "    for episodes in range(1,100000):\n",
    "        q1_weight, q2_weight, q1_target_weight, q2_target_weight = R.get_weights()\n",
    "        [worker.set_weights.remote(q1_weight, q2_weight, q1_target_weight, q2_target_weight) for worker in workers]\n",
    "        ops = [worker.rollout.remote(memory, R) for worker in workers]\n",
    "        rollout_vals = ray.get(ops)\n",
    "\n",
    "        for score_, step_, small_buffer in rollout_vals:\n",
    "            memory.buffer.extend(list(small_buffer))\n",
    "            score += score_ / n_workers\n",
    "\n",
    "        if score_ > bestsc:\n",
    "            bestsc = score_ # best among workers\n",
    "        \n",
    "        if memory.size() > 30000:\n",
    "            for i in range(gradient_update):\n",
    "                mini_batch = memory.sample(batch_size)\n",
    "                td_target = get_target(R.pi, R.q1_target, R.q2_target, mini_batch)\n",
    "                R.q1.train_q(td_target, mini_batch)\n",
    "                R.q2.train_q(td_target, mini_batch)\n",
    "                R.pi.train_p(R.q1, R.q2, mini_batch)\n",
    "                R.q1.soft_update(R.q1_target)\n",
    "                R.q2.soft_update(R.q2_target)\n",
    "            \n",
    "        if episodes % print_interval==0 and episodes!=0:\n",
    "            print(\"number of episode :{}, avg score :{:.1f}, best score :{:.1f}, avg step :{:.1f}, alpha:{:.4f}\".format(episodes, score/print_interval, best_score, step/print_interval, pi.log_alpha.exp()))\n",
    "            if not os.path.exists(\"weights_{}\".format(folder)):\n",
    "                os.mkdir(\"weights_{}\".format(folder))\n",
    "            torch.save(pi.state_dict(), 'weights_{}/model_weights_{}.pth'.format(folder,episodes))\n",
    "            score = 0\n",
    "            step = 0\n",
    "\n",
    "        if episodes >= 2500:\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop and render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_ant_render()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a336408ed5cf483d0b6697114c3c65340b20f5cdc6d74b72c0de63404a47289a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
